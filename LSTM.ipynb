{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9lltDD5uvzn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, SimpleRNN, GRU, Bidirectional, RNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from itertools import product\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIHJ0O4PuyMz"
      },
      "source": [
        "def remove_commas(data):\n",
        "  new_data = []\n",
        "  for row in data:\n",
        "    row_ = []\n",
        "    for elem in row:\n",
        "      if ',' in str(elem):\n",
        "        elem = elem.replace(',', '')\n",
        "      elem = float(elem)\n",
        "      row_.append(elem)\n",
        "    new_data.append(row_)\n",
        "\n",
        "  return np.array(new_data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i5n99nFuziU"
      },
      "source": [
        "# read in train data\n",
        "data = pd.read_csv('drive/MyDrive/Colab Notebooks/Google_Stock_Price_Train.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "colnames = list(data.columns[1:])\n",
        "\n",
        "# divide train and val and scale them\n",
        "condition = (data['Date'] > '2016-01-01') & (data['Date'] <= '2016-12-31')\n",
        "\n",
        "data_2012_2015 = data.loc[~condition]\n",
        "train_raw = remove_commas(data_2012_2015.drop(columns=['Date']).copy().values)\n",
        "train_scaled = scaler.fit_transform(train_raw)\n",
        "\n",
        "data_2016 = data.loc[condition]\n",
        "val_raw = remove_commas(data_2016.drop(columns=['Date']).copy().values)\n",
        "val_scaled = scaler.transform(val_raw) \n",
        "\n",
        "nfeatures = train_scaled.shape[1]\n",
        "\n",
        "# read in test data and scale them\n",
        "test_data = pd.read_csv('drive/MyDrive/Colab Notebooks/Google_Stock_Price_Test.csv')\n",
        "test_raw = remove_commas(test_data.drop(columns=['Date']).copy().values)\n",
        "test_scaled = scaler.transform(test_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OrZequDwH_X"
      },
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYZuLmr-vZF_"
      },
      "source": [
        "# Visualisation of Google's Stock Price\n",
        "\n",
        "full_data_raw = np.vstack((train_raw, val_raw, test_raw))\n",
        "\n",
        "fig, axs = plt.subplots(5,1, figsize = (10,15), sharex = True)\n",
        "for i in range(len(colnames)):\n",
        "  axs[i].plot(full_data_raw[:,i], color = 'blue')\n",
        "  axs[i].set_title(colnames[i])\n",
        "  if i == len(colnames)-1:\n",
        "    axs[i].set_ylabel('Number of shares')\n",
        "  else:\n",
        "    axs[i].set_ylabel('Price ($)')\n",
        "axs[-1].set_xlabel('Time')\n",
        "#plt.tight_layout()\n",
        "plt.savefig('full_data.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzpYTI9Ju4M0"
      },
      "source": [
        "# function to transform data into a supervised learning problem\n",
        "\n",
        "def prepare_data(train_scaled, val_scaled, test_scaled, window_size):\n",
        "\n",
        "  nfeatures = train_scaled.shape[0]\n",
        "\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "\n",
        "  for i in range(len(train_scaled)-window_size):\n",
        "    X_train.append(train_scaled[i:i+window_size, :])\n",
        "    y_train.append(train_scaled[i+window_size,:])\n",
        "\n",
        "  X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "  X_val = []\n",
        "  y_val = []\n",
        "\n",
        "  full_val_set = np.vstack((train_scaled[-window_size:], val_scaled))\n",
        "  for i in range(len(full_val_set)-window_size):\n",
        "    X_val.append(full_val_set[i:i+window_size, :])\n",
        "    y_val.append(full_val_set[i+window_size,:])\n",
        "\n",
        "  X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "  # making predictions and visualise\n",
        "  full_test_set = np.vstack((val_scaled[-window_size:, :], test_scaled))\n",
        "\n",
        "  X_test = []\n",
        "\n",
        "  for i in range(len(full_test_set)-window_size):\n",
        "    X_test.append(full_test_set[i:i+window_size, :])\n",
        "\n",
        "  X_test = np.array(X_test)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68fBRpbavNQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZynaSzXbvqDI"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC9OuWy1vq2k"
      },
      "source": [
        "def create_model(parameters, window_size, nfeatures, model_type, bidirectional):\n",
        "  units, nlayers, dropout_rate, learning_rate = parameters\n",
        "  model = Sequential()\n",
        "\n",
        "  if model_type == 'LSTM':\n",
        "    for i in range(nlayers):\n",
        "      if bidirectional:\n",
        "        model.add(Bidirectional(LSTM(units = units, return_sequences = True, input_shape = (window_size, nfeatures))))\n",
        "      else:\n",
        "         model.add(LSTM(units = units, return_sequences = True, input_shape = (window_size, nfeatures)))\n",
        "      model.add(Dropout(0.2))\n",
        "    \n",
        "    if bidirectional:\n",
        "      model.add(Bidirectional(LSTM(units = units)))\n",
        "    else:\n",
        "      model.add(LSTM(units = units))\n",
        "    model.add(Dropout(0.2)) \n",
        "\n",
        "  elif model_type == 'GRU':\n",
        "    for i in range(nlayers):\n",
        "      if bidirectional:\n",
        "        model.add(Bidirectional(GRU(units = units, return_sequences = True, input_shape = (window_size, nfeatures))))\n",
        "      else:\n",
        "         model.add(GRU(units = units, return_sequences = True, input_shape = (window_size, nfeatures)))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    if bidirectional:\n",
        "      model.add(Bidirectional(GRU(units = units)))\n",
        "    else:\n",
        "      model.add(GRU(units = units))\n",
        "    model.add(Dropout(0.2)) \n",
        "\n",
        "  elif model_type == 'SimpleRNN':\n",
        "    for i in range(nlayers):\n",
        "      if bidirectional:\n",
        "        model.add(Bidirectional(SimpleRNN(units = units, return_sequences = True, input_shape = (window_size, nfeatures))))\n",
        "      else:\n",
        "         model.add(SimpleRNN(units = units, return_sequences = True, input_shape = (window_size, nfeatures)))\n",
        "      model.add(Dropout(0.2))\n",
        "    \n",
        "    if bidirectional:\n",
        "      model.add(Bidirectional(SimpleRNN(units = units)))\n",
        "    else:\n",
        "      model.add(SimpleRNN(units = units))\n",
        "    model.add(Dropout(0.2)) \n",
        "\n",
        "  model.add(Dense(units = nfeatures))\n",
        "\n",
        "  opt = Adam(learning_rate = learning_rate)\n",
        "  model.compile(optimizer = opt, loss = 'mean_squared_error')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjT3bYgpvslI"
      },
      "source": [
        "data = []\n",
        "parameters = (32, 2, 0.2, 0.01)\n",
        "window_size, nfeatures = 60, train_raw.shape[1]\n",
        "\n",
        "for model_type in ['SimpleRNN', 'GRU', 'LSTM']:\n",
        "  for bidirectional in [False, True]:\n",
        "    predicted_val_data = []\n",
        "    predicted_test_data = []\n",
        "    rmse_data = []\n",
        "\n",
        "    for ntime in range(5):\n",
        "      # run each model 5 times\n",
        "      tf.keras.backend.clear_session()\n",
        "      model = create_model(parameters, window_size, nfeatures, model_type, bidirectional)\n",
        "\n",
        "      stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 50,\n",
        "                                                restore_best_weights = True)\n",
        "\n",
        "      history = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100, \n",
        "                          batch_size = 32, verbose = 0, callbacks = [stop_early])\n",
        "      \n",
        "      predicted_val = model.predict(X_val)\n",
        "      predicted_val = scaler.inverse_transform(predicted_val)\n",
        "\n",
        "      predicted = model.predict(X_test)\n",
        "      predicted = scaler.inverse_transform(predicted)\n",
        "\n",
        "      rmse = mean_squared_error(test_raw, predicted, squared = False)\n",
        "      rmse_data.append(rmse)\n",
        "\n",
        "      predicted_val_data.append(predicted_val)\n",
        "      predicted_test_data.append(predicted)\n",
        "      \n",
        "    mean_predicted_val = np.mean(np.dstack(predicted_val_data), axis = -1)\n",
        "    mean_predicted_test = np.mean(np.dstack(predicted_test_data), axis = -1)\n",
        "    mean_rmse = np.mean(rmse)\n",
        "\n",
        "    data.append((mean_predicted_val, mean_predicted_test, mean_rmse))\n",
        "\n",
        "    for i in range(nfeatures):\n",
        "      plt.plot(val_raw[:,i], label = 'actual')\n",
        "      plt.plot(mean_predicted_val[:,i], label = 'predicted')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    for i in range(nfeatures):\n",
        "      plt.plot(test_raw[:,i], label = 'actual')\n",
        "      plt.plot(mean_predicted_test[:,i], label = 'predicted')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    print('Average RMSE: ', mean_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoSd7gWBv8Dp"
      },
      "source": [
        "i = 0\n",
        "path = 'drive/MyDrive/Colab Notebooks/results'\n",
        "for name in ['SimpleRNN', 'GRU', 'LSTM']:\n",
        "  for b in [False, True]:\n",
        "    np.save(path + '{}_{}_predicted_val.npy'.format(name, str(b)), data[i][0])\n",
        "    np.save(path +'{}_{}_predicted_test.npy'.format(name, str(b)), data[i][1])\n",
        "    np.save(path +'{}_{}_mse.npy'.format(name, str(b)), data[i][2])\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiVMEdtWwV-w"
      },
      "source": [
        "path = 'drive/MyDrive/Colab Notebooks/results/'\n",
        "results_dict = {'SimpleRNN_False': [],\n",
        "               'SimpleRNN_True': [],\n",
        "               'GRU_False': [],\n",
        "               'GRU_True': [],\n",
        "               'LSTM_False': [],\n",
        "               'LSTM_True': []}\n",
        "\n",
        "for key in results_dict:\n",
        "  path = 'drive/MyDrive/Colab Notebooks/results/' + key + '_'\n",
        "  results_dict[key] = [np.load(path+'predicted_val.npy'), np.load(path+'predicted_test.npy'), np.load(path+'mse.npy')]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkKNzAPvw2zb"
      },
      "source": [
        "# checking all RMSE\n",
        "\n",
        "all_rmse = []\n",
        "for key in results_dict:\n",
        "  all_rmse.append((key, results_dict[key][-1]))\n",
        "\n",
        "all_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HU4GtkwgOs"
      },
      "source": [
        "# get result of best model, which is BRNN GRU\n",
        "gru_true_test_predicted = results_dict['GRU_True'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1AAC3x0wjKC"
      },
      "source": [
        "fig, axs = plt.subplots(3,2, figsize = (12,14))\n",
        "n = 0\n",
        "for i in range(3):\n",
        "  for j in range(2):\n",
        "    if n < len(colnames):\n",
        "      axs[i,j].plot(test_raw[:,n], color = 'blue', label = 'Actual')\n",
        "      axs[i,j].plot(gru_true_test_predicted[:,n], color = 'red', label = 'Predicted')\n",
        "      axs[i,j].set_title(colnames[n])\n",
        "      if n == len(colnames)-1:\n",
        "        axs[i,j].set_ylabel('Number of shares')\n",
        "      else:\n",
        "        axs[i,j].set_ylabel('Price ($)')\n",
        "      axs[i,j].set_xlabel('Time')\n",
        "      axs[i,j].legend()\n",
        "    n += 1\n",
        "axs[-1,-1].axis('off')\n",
        "plt.savefig('gru_true.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F2fFdbMwUbM"
      },
      "source": [
        "# Experiment 2\n",
        "\n",
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBz_PU-wsSG"
      },
      "source": [
        "window_size = [20, 60]\n",
        "batch_size = [32, 128]\n",
        "learning_rate = [1e-2, 1e-3, 1e-4]\n",
        "optimizer = [0,1,2]\n",
        "\n",
        "configs = list(product(window_size, batch_size, learning_rate, optimizer))\n",
        "np.random.seed(69)\n",
        "np.random.shuffle(configs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3vLwb_EwsWm"
      },
      "source": [
        "data = []\n",
        "nlayers, units = 2, 32 # more like 3\n",
        "dropout_rate = 0.2\n",
        "ntime = 1\n",
        "\n",
        "for each_config in configs[1:]:\n",
        "  print(ntime)\n",
        "  tf.keras.backend.clear_session()\n",
        "  window_size, batch_size, learning_rate, optimizer = each_config\n",
        "\n",
        "  opt_name = ['SGD', 'RMSprop', 'Adam']\n",
        "  print('Window size: {}, Batch size: {}, lr: {}, optimizer = {}'.format(window_size, batch_size,\n",
        "                                                                         learning_rate, opt_name[optimizer]))\n",
        "\n",
        "  X_train, y_train, X_val, y_val, X_test = prepare_data(train_scaled, val_scaled, test_scaled, window_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  for i in range(nlayers):\n",
        "    model.add(Bidirectional(GRU(units = units, return_sequences = True, input_shape = (window_size, nfeatures))))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "  model.add(Bidirectional(GRU(units = units)))\n",
        "  model.add(Dropout(dropout_rate)) \n",
        "\n",
        "  model.add(Dense(units = nfeatures))\n",
        "\n",
        "  opt = SGD(learning_rate)\n",
        "  if optimizer == 1:\n",
        "    opt = RMSprop(learning_rate)\n",
        "  elif optimizer == 2:\n",
        "    opt = Adam(learning_rate = learning_rate)\n",
        "\n",
        "  model.compile(optimizer = opt, loss = 'mean_squared_error')\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 20,\n",
        "                                                restore_best_weights = True)\n",
        "  \n",
        "  history = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 200, \n",
        "                          batch_size = batch_size, verbose = 0, callbacks = [stop_early])\n",
        "\n",
        "  plt.plot(history.history['loss'], label = 'train')\n",
        "  plt.plot(history.history['val_loss'], label = 'validation')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "      \n",
        "  predicted_val = model.predict(X_val)\n",
        "  predicted_val = scaler.inverse_transform(predicted_val)\n",
        "  for i in range(nfeatures):\n",
        "    plt.plot(val_raw[:,i], label = 'actual')\n",
        "    plt.plot(predicted_val[:,i], label = 'predicted')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  rmse = mean_squared_error(val_raw, predicted_val, squared = False)\n",
        "  print('RMSE: ', rmse)\n",
        "  data.append(rmse)\n",
        "  ntime += 1\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPFsCokxxFpg"
      },
      "source": [
        "# Window size: 20, Batch size: 32, lr: 0.001, optimizer = Adam\n",
        "best_window_size, best_batch_size, best_lr = 20, 32, 0.001\n",
        "best_opt = Adam(best_lr)\n",
        "nlayers, units, dropout_rate = 2, 32, 0.2\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test = prepare_data(train_scaled, val_scaled, test_scaled, best_window_size)\n",
        "\n",
        "# finding the best epoch\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "for i in range(nlayers):\n",
        "  model.add(Bidirectional(GRU(units = units, return_sequences = True, input_shape = (best_window_size, nfeatures))))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "\n",
        "model.add(Bidirectional(GRU(units = units)))\n",
        "model.add(Dropout(dropout_rate)) \n",
        "\n",
        "model.add(Dense(units = nfeatures))\n",
        "\n",
        "model.compile(optimizer = best_opt, loss = 'mean_squared_error')\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 20,\n",
        "                                                restore_best_weights = True)\n",
        "history = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 200, batch_size = best_batch_size, verbose = 0,\n",
        "                    callbacks = [stop_early])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4RKUvLcxT3J"
      },
      "source": [
        "val_loss = history.history['val_loss']\n",
        "best_epoch = val_loss.index(min(val_loss)) + 1\n",
        "best_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wKSb7mmyQny"
      },
      "source": [
        "## Using the best epoch and the hyperparameters found above and retrain model using just the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdxqqKkZxWQ0"
      },
      "source": [
        "# using the best epoch and the hyperparameters found above\n",
        "# retrain model using just the train set\n",
        "\n",
        "# Window size: 20, Batch size: 32, lr: 0.001, optimizer = Adam\n",
        "predicted_ls = []\n",
        "rmse_ls = []\n",
        "X_train, y_train, X_val, y_val, X_test = prepare_data(train_scaled, val_scaled, test_scaled, best_window_size)\n",
        "\n",
        "for ntime in range(5):\n",
        "  # run 5 times\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  model = Sequential()\n",
        "  for i in range(nlayers):\n",
        "    model.add(Bidirectional(GRU(units = units, return_sequences = True, input_shape = (best_window_size, nfeatures))))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "  model.add(Bidirectional(GRU(units = units)))\n",
        "  model.add(Dropout(dropout_rate)) \n",
        "\n",
        "  model.add(Dense(units = nfeatures))\n",
        "\n",
        "  model.compile(optimizer = best_opt, loss = 'mean_squared_error')\n",
        "\n",
        "  history = model.fit(X_train, y_train, epochs = best_epoch, batch_size = best_batch_size, verbose = 0)\n",
        "        \n",
        "  predicted = model.predict(X_test)\n",
        "  predicted = scaler.inverse_transform(predicted)\n",
        "  predicted_ls.append(predicted)\n",
        "\n",
        "  rmse = mean_squared_error(test_raw, predicted, squared = False)\n",
        "  rmse_ls.append(rmse)\n",
        "  print(rmse)\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNMQUjutxWVY"
      },
      "source": [
        "mean_predicted = np.mean(np.dstack(predicted_ls), axis = -1)\n",
        "\n",
        "fig, axs = plt.subplots(3,2, figsize = (12,14))\n",
        "n = 0\n",
        "for i in range(3):\n",
        "  for j in range(2):\n",
        "    if n < len(colnames):\n",
        "      axs[i,j].plot(test_raw[:,n], color = 'blue', label = 'Actual')\n",
        "      axs[i,j].plot(mean_predicted[:,n], color = 'red', label = 'Predicted')\n",
        "      axs[i,j].set_title(colnames[n])\n",
        "      if n == len(colnames)-1:\n",
        "        axs[i,j].set_ylabel('Number of shares')\n",
        "      else:\n",
        "        axs[i,j].set_ylabel('Price ($)')\n",
        "      axs[i,j].set_xlabel('Time')\n",
        "      axs[i,j].legend()\n",
        "    n += 1\n",
        "axs[-1,-1].axis('off')\n",
        "\n",
        "plt.savefig('retrain_train.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQP6PPPrxpbh"
      },
      "source": [
        "mean_rmse = np.mean(rmse_ls)\n",
        "mean_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chs95sS1xsHd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9c8pcKzyZZo"
      },
      "source": [
        "## Using the best epoch and the hyperparameters found above and retrain model using the train and val set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N91fo60txsQP"
      },
      "source": [
        "train_val_scaled = np.vstack((train_scaled, val_scaled))\n",
        "full_test_set = np.vstack((train_val_scaled[-best_window_size:, :], test_scaled))\n",
        "\n",
        "X_train_val = []\n",
        "y_train_val = []\n",
        "for i in range(len(train_val_scaled)-best_window_size):\n",
        "  X_train_val.append(train_val_scaled[i:i+best_window_size, :])\n",
        "  y_train_val.append(train_val_scaled[i+best_window_size,:])\n",
        "X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n",
        "X_train_val.shape\n",
        "\n",
        "X_test = []\n",
        "for i in range(len(full_test_set)-best_window_size):\n",
        "  X_test.append(full_test_set[i:i+best_window_size, :])\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dmFfuczxyCA"
      },
      "source": [
        "\n",
        "predicted_ls1 = []\n",
        "rmse_ls1 = []\n",
        "\n",
        "for ntime in range(5):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  model = Sequential()\n",
        "  for i in range(nlayers):\n",
        "    model.add(Bidirectional(GRU(units = units, return_sequences = True, input_shape = (best_window_size, nfeatures))))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "  model.add(Bidirectional(GRU(units = units)))\n",
        "  model.add(Dropout(dropout_rate)) \n",
        "\n",
        "  model.add(Dense(units = nfeatures))\n",
        "\n",
        "  model.compile(optimizer = best_opt, loss = 'mean_squared_error')\n",
        "\n",
        "  history = model.fit(X_train_val, y_train_val, epochs = best_epoch, batch_size = best_batch_size, verbose = 0)\n",
        "\n",
        "  plt.plot(history.history['loss'], label = 'train')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "        \n",
        "  predicted = model.predict(X_test)\n",
        "  predicted = scaler.inverse_transform(predicted)\n",
        "  predicted_ls1.append(predicted)\n",
        "\n",
        "  rmse = mean_squared_error(test_raw, predicted, squared = False)\n",
        "  rmse_ls1.append(rmse)\n",
        "  print(rmse)\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PBN6cD0x7hv"
      },
      "source": [
        "mean_predicted1 = np.mean(np.dstack(predicted_ls1), axis = -1)\n",
        "\n",
        "fig, axs = plt.subplots(3,2, figsize = (12,14))\n",
        "n = 0\n",
        "for i in range(3):\n",
        "  for j in range(2):\n",
        "    if n < len(colnames):\n",
        "      axs[i,j].plot(test_raw[:,n], color = 'blue', label = 'Actual')\n",
        "      axs[i,j].plot(mean_predicted1[:,n], color = 'red', label = 'Predicted')\n",
        "      axs[i,j].set_title(colnames[n])\n",
        "      if n == len(colnames)-1:\n",
        "        axs[i,j].set_ylabel('Number of shares')\n",
        "      else:\n",
        "        axs[i,j].set_ylabel('Price ($)')\n",
        "      axs[i,j].set_xlabel('Time')\n",
        "      axs[i,j].legend()\n",
        "    n += 1\n",
        "axs[-1,-1].axis('off')\n",
        "\n",
        "plt.savefig('both_train_val.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW0CYJnAyC2r"
      },
      "source": [
        "mean_rmse1 = np.mean(rmse_ls1)\n",
        "mean_rmse1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}